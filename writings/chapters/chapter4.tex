\label{ch:four}
\label{ch:four}

\section{Mutual information}\label{mutInfo}
	The mutual information measures the amount of information that $X$ and $Y$ \textit{share}.
    It can be used as a measure of correlation (or dependency) between random variables.
    Intuitively we can use the mutual information --- along with a check of privacy against Eve --- to measure how much shared key two random variables $X$ and $Y$ can hold.
    \begin{definition}
	Let $X$ and $Y$ be two jointly distributed random variables. Then the mutual information of the random variables is the relative entropy --- a measure of distance between probability distributions --- between the joint distribution $P_{XY}(x,y)$ and the product distribution $P_X(x)\cdot P_Y(y)$.
	\begin{equation}
		\I(X;Y) = \sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}} p(x,y) \log\left(\frac{p(x,y)}{p(x)p(y)}\right) 
	\end{equation}
	or equivalently, showing its relation to the entropies of the random variables
	\begin{equation}
		\I(X;Y) = \Ent(X) - \Ent(X|Y) = \Ent(X,Y) - \Ent(X|Y) - \Ent(Y|X) = \Ent(Y) - \Ent(Y|X)
	\end{equation}
	This relation can be seen more directly in Fig. \ref{fig:mutual_info}.
    \end{definition} 
	Mutual information is nonnegative and bounded by the entropy of random variable $X$
	\begin{equation}
		0 \leq \I(X;Y) \leq \min \left(\Ent(X), \Ent(Y)\right)
	\end{equation}
	In this sense the mutual information can also be interpreted as how much information $X$ gives about $Y$, thus being bounded by its own entropy.
	\begin{figure}[ht]
		\centering
		\input{images/mutual-info}
		\caption{Representation of mutual information $\I(X;Y)$ in relation with entropies $\Ent(X)$ and $\Ent(Y)$ and joint entropy $\Ent(X,Y)$ of the random variables .
		\label{fig:mutual_info}}
	\end{figure}	
	
\section{An eavesdropper that can choose the best channel to listen to}
Additional information on a third random variable $Z$ can increase or decrease the mutual information \cite{CT12}.
    The \emph{conditional mutual information} $\I(X;Y|Z)$ is the expected value of the mutual information of $X$ and $Y$ given a realization of a third variable $Z$.
    In a context of key exchange, we can interpret this as the remaining correlation between honest parties after the observations of an attacker Eve.
    What if Eve tried to minimize this, i.e. tried to find the best viewpoint possible over the communication between Alice and Bob?
    \begin{definition}\cite{MW99, RW03}
    	Let $P_{XYZ}$ be a discrete probability distribution. Then the intrinsic information between $X$ and $Y$ given $Z$ is
    \begin{equation} \label{intrininfo}
    	\intrinfo{X}{Y}{Z}:= \inf_{Z\rightarrow \bar{Z}} \I(X;Y | \bar{Z})
    \end{equation}
    \end{definition}
    The infimum is taken over all possible channels applied to $Z$ (the choice of a channel can be seen as the choice of a point of view for Eve).
    
    The intrinsic information is an upper bound to the secret-key rate, although not tight \cite{RW03}. 
    \begin{equation} \label{eq:bkeyinfo}
    	\keyrate{X}{Y}{Y} \leq \intrinfo{X}{Y}{Y}
    \end{equation}
    Refer to the next chapter to see an analysis of the gap between the two measures.
    The amount of secret bits Alice and Bob can extract from the distribution is then bounded by how much the attacker Eve can disrupts their conditional mutual correlation.
    Intrinsic information is also a lower bound to another measure, \emph{information of formation}, which is the amount of initial secret bits between Alice and Bob required to create the distribution $P_{XYZ}$ with LOPC.
    
   
\section{When correlation is unusable}
Setting the bound in Eq. \ref{eq:bkeyinfo} we can see that not always factoring out the adversary can be enough to be able to produce a key.
For example we could have 
\begin{align*}
	\keyrate{X}{Y}{Z} & = 0 \\
	\intrinfo{X}{Y}{Z} & > 0 
\end{align*}
meaning that there exists some sort of mutual correlation between Alice and Bob, but they share no key.
Whether this case is possible is the question of bound information expressed at the beginning of this work.
\begin{definition}\cite{GisWolf00, RW03} 
Let $P_{XYZ}$ be a joint probability distribution for parties Alice, Bob and Eve.
For such distribution let 
\begin{equation}
	\intrinfo{X}{Y}{Z} > 0
\end{equation}
and 
\begin{equation}
\keyrate{X}{Y}{Z} = 0
\end{equation}
hold.
Then $P_{XYZ}$ is said to have \emph{bound information}.
\end{definition}

Recalling the intuition from quantum mechanics, we now pose the case of the existence of bound entanglement.
As stated before (section \ref{distillation}), quantum distillation extract from an entangled mixed state a set of quasi-pure entangled states.
Pure entangled states can be used as a resource to produce a key for Alice and Bob \cite{Ekert91}.
There are, furthermore, entangled mixed states that are non-distillable, i.e. no pure entanglement can be extracted from them \cite{3H98}.
Bound entanglement is a kind of correlation between Alice and Bob --- that can become inaccessible to Eve --- but nevertheless of no use for generating a secret (quantum) key.
So in the quantum regime this questions has already been answered.
	
	
	
		
